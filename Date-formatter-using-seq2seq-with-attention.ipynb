{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing Faker to generate dates","metadata":{}},{"cell_type":"code","source":"!pip install Faker ","metadata":{"execution":{"iopub.status.busy":"2021-10-08T14:09:34.584251Z","iopub.execute_input":"2021-10-08T14:09:34.584477Z","iopub.status.idle":"2021-10-08T14:09:41.218106Z","shell.execute_reply.started":"2021-10-08T14:09:34.584451Z","shell.execute_reply":"2021-10-08T14:09:41.217140Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Importing necessary modules","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\nfrom tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import load_model, Model\nimport tensorflow.keras.backend as K\nimport numpy as np\n\nfrom faker import Faker\nimport random\nfrom tqdm import tqdm\nfrom babel.dates import format_date\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2021-10-08T14:10:30.890154Z","iopub.execute_input":"2021-10-08T14:10:30.891084Z","iopub.status.idle":"2021-10-08T14:10:30.902718Z","shell.execute_reply.started":"2021-10-08T14:10:30.891046Z","shell.execute_reply":"2021-10-08T14:10:30.902053Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Defining some functions for data generating and preprocessing","metadata":{}},{"cell_type":"code","source":"#loading faker to generate fake dataset\nfake = Faker()\nFaker.seed(1655)\nrandom.seed(1655)\n# Defining format of the data we would like to generate\nformats = ['short','medium','long', 'full','full','full','full','full','full','full','full',\n           'full','full','d MMM YYY', 'd MMMM YYY','dd MMM YYY','dd MMM, YYY','d MMMM, YYY',\n           'dd, MMM YYY','d MM YY','d MMMM YYY','MMMM d YYY','MMMM d, YYY','dd.MM.YY']\n\n\ndef generate_date():\n    \"\"\"\n        Loads some fake dates \n    \"\"\"\n    date_obj = fake.date_object()\n    try:\n        human_readable = format_date(date_obj, format=random.choice(formats),  locale='en_US')\n        human_readable = human_readable.lower()\n        human_readable = human_readable.replace(',','')\n        machine_readable = date_obj.isoformat()\n    except AttributeError as e:\n        return None, None, None\n    return human_readable, machine_readable, date_obj\n\n\n\ndef load_dataset(m):\n    \"\"\"\n    m: the number of examples to generate\n    \"\"\"\n    input_vocab = set()\n    output_vocab = set()\n    dataset = []\n    Tx = 30  #maximum input date length\n    for i in tqdm(range(m)):  #tqdm is used to visualize progress bar through iteration\n        input_date, output_date, _ = generate_date()\n        if input_date is not None:\n            dataset.append((input_date, output_date))\n            input_vocab.update(tuple(input_date))\n            output_vocab.update(tuple(output_date))\n    input_vocab = dict(zip(sorted(input_vocab) + ['<unk>', '<pad>'], \n                     list(range(len(input_vocab) + 2))))\n    inv_output_vocab = dict(enumerate(sorted(output_vocab)))\n    output_vocab = {v:k for k,v in inv_output_vocab.items()}\n    return dataset, input_vocab, output_vocab, inv_output_vocab\n\n\n\ndef string_to_int(string, length, vocab):\n    \"\"\"\n    string -- input string, e.g. 'Wed 10 Jul 2007'\n    length -- the number of time steps you'd like, determines if the output will be padded or cut\n    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n    \"\"\"\n    string = string.lower()\n    string = string.replace(',','')\n    if len(string) > length:\n        string = string[:length]\n    rep = list(map(lambda x: vocab.get(x, \"<unk>\"), string))\n    if len(string) < length:\n        rep = rep + [vocab['<pad>']] * (length - len(string))\n    return rep\n\n\ndef int_to_string(ints, inv_vocab):\n    \"\"\"\n    ints -- list of integers representing indexes in the vocabulary\n    inv_vocab -- dictionary mapping indexes to characters \n    \"\"\"\n    l = [inv_vocab[i] for i in ints]\n    return l\n\n\n\ndef preprocess_data(dataset, input_vocab, output_vocab, Tx, Ty):\n    X, Y = zip(*dataset)\n    X = np.array([string_to_int(string, Tx, input_vocab) for string in X])\n    Y = np.array([string_to_int(string, Ty, output_vocab) for string in Y])\n    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocab)), X)))\n    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(output_vocab)), Y)))\n    return X, Y, Xoh, Yoh","metadata":{"execution":{"iopub.status.busy":"2021-10-08T14:50:47.414485Z","iopub.execute_input":"2021-10-08T14:50:47.414743Z","iopub.status.idle":"2021-10-08T14:50:47.451699Z","shell.execute_reply.started":"2021-10-08T14:50:47.414713Z","shell.execute_reply":"2021-10-08T14:50:47.451060Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"## Loading dataset","metadata":{}},{"cell_type":"code","source":"Tx = 30 #maximum input date length\nTy = 10 #maximum output date length (0 to 9 and -)\nm = 20000 #training data size\ndataset, input_vocab, output_vocab, inv_output_vocab = load_dataset(m)\nX, Y, Xoh, Yoh = preprocess_data(dataset, input_vocab, output_vocab, Tx, Ty)\n\nprint(\"X.shape:\", X.shape)\nprint(\"Y.shape:\", Y.shape)\nprint(\"Xoh.shape:\", Xoh.shape)\nprint(\"Yoh.shape:\", Yoh.shape)\nprint(\"Some samples:\")\ndataset[:5]","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:54:48.847104Z","iopub.execute_input":"2021-10-08T15:54:48.847716Z","iopub.status.idle":"2021-10-08T15:54:50.681212Z","shell.execute_reply.started":"2021-10-08T15:54:48.847677Z","shell.execute_reply":"2021-10-08T15:54:50.680403Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"## Custom softmax function that calculates softmax across defined axis","metadata":{}},{"cell_type":"code","source":"def softmax(x, axis=1):  #axis 1 is time axis\n    num_dim = K.ndim(x)\n    if num_dim == 2:\n        return K.softmax(x)\n    elif num_dim > 2:\n        e = K.exp(x - K.max(x, axis=axis, keepdims=True)) #max is subtracted for numerical stability.\n        #this is subtuction is applied in both numerator and denominator and they cancel out each other.\n        s = K.sum(e, axis=axis, keepdims=True)\n        return e/s\n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:54:58.884871Z","iopub.execute_input":"2021-10-08T15:54:58.885547Z","iopub.status.idle":"2021-10-08T15:54:58.892998Z","shell.execute_reply.started":"2021-10-08T15:54:58.885509Z","shell.execute_reply":"2021-10-08T15:54:58.892224Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"## Global layers used in attention calculation","metadata":{}},{"cell_type":"code","source":"'''Global layers used in attention calculation'''\nrepeatVecLayer = RepeatVector(Tx)\nconcatLayer = Concatenate(axis=-1)\ndenseLayer1 = Dense(10, activation = \"tanh\")\ndenseLayer2 = Dense(1, activation = \"relu\")\nactivationLayer = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1)\ndotLayer = Dot(axes = 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:06.102360Z","iopub.execute_input":"2021-10-08T15:55:06.102613Z","iopub.status.idle":"2021-10-08T15:55:06.111970Z","shell.execute_reply.started":"2021-10-08T15:55:06.102586Z","shell.execute_reply":"2021-10-08T15:55:06.111168Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"## Function to calculate attention for each decoder time stamp","metadata":{}},{"cell_type":"code","source":"def one_step_attention(h, s_prev):\n    \"\"\"\n    h --> hidden states of encoder of shape (m, Tx, 2*h_dim)\n    s_prev --> previous hidden state of decoder of shape (m, s_dim)\n    \"\"\"\n    # Repeator repeats s_prev to be of shape (m, Tx, s_dim) \n    s_prev = repeatVecLayer(s_prev)  \n    concat = concatLayer([h, s_prev])  #concat (m, Tx, s_dim + 2*h_dim)\n    e = denseLayer1(concat)  #e (m, Tx, 10) \n    e = denseLayer2(e)  #e (m, Tx, 1)\n    alphas = activationLayer(e)  #alphas (m, Tx, 1)\n    context = dotLayer([alphas, h])   #context (m, 1, 2*h_dim) \n    return context","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:08.084010Z","iopub.execute_input":"2021-10-08T15:55:08.084321Z","iopub.status.idle":"2021-10-08T15:55:08.091218Z","shell.execute_reply.started":"2021-10-08T15:55:08.084279Z","shell.execute_reply":"2021-10-08T15:55:08.090466Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"## Global layers used in decoder section","metadata":{}},{"cell_type":"code","source":"'''Global layers used in decoder section'''\nh_dim = 32\ns_dim = 64\ndecoder_LSTM = LSTM(s_dim, return_state = True)\noutput_layer = Dense(len(output_vocab), activation=softmax)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:11.527900Z","iopub.execute_input":"2021-10-08T15:55:11.528419Z","iopub.status.idle":"2021-10-08T15:55:11.538170Z","shell.execute_reply.started":"2021-10-08T15:55:11.528385Z","shell.execute_reply":"2021-10-08T15:55:11.537201Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"## Defining model","metadata":{}},{"cell_type":"code","source":"def model(Tx, Ty, h_dim, s_dim, input_vocab_size, output_vocab_size):\n    \n    X = Input(shape=(Tx, input_vocab_size))\n    s0 = Input(shape=(s_dim,), name='s0')\n    c0 = Input(shape=(s_dim,), name='c0')\n    s = s0  #s (m, s_dim)\n    c = c0\n    outputs = []\n    h = Bidirectional(LSTM(h_dim, return_sequences=True),input_shape=(Tx, h_dim))(X)  \n    # Iterate for Ty steps\n    for t in range(Ty):\n        context = one_step_attention(h, s)\n        s, _, c = decoder_LSTM(context, initial_state = [s, c]) \n        output = output_layer(s) #output (m, output_vocab_size)\n        outputs.append(output) #outputs final dims (Ty, m, output_vocab_size)\n        \n    model = Model(inputs=[X, s0, c0], outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:13.667214Z","iopub.execute_input":"2021-10-08T15:55:13.667483Z","iopub.status.idle":"2021-10-08T15:55:13.674774Z","shell.execute_reply.started":"2021-10-08T15:55:13.667454Z","shell.execute_reply":"2021-10-08T15:55:13.673769Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"## Creating model","metadata":{}},{"cell_type":"code","source":"model = model(Tx, Ty, h_dim, s_dim, len(input_vocab), len(output_vocab))\noptimizer = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) \nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:16.723784Z","iopub.execute_input":"2021-10-08T15:55:16.724350Z","iopub.status.idle":"2021-10-08T15:55:19.592732Z","shell.execute_reply.started":"2021-10-08T15:55:16.724314Z","shell.execute_reply":"2021-10-08T15:55:19.592067Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"## Training model","metadata":{}},{"cell_type":"code","source":"s0 = np.zeros((m, s_dim))\nc0 = np.zeros((m, s_dim))\n#since the shape of model output (Ty, m, output_vocab_size), we need to swap Yoh axes to be alined with model output\n#Yoh has shape (m, Ty, output_vocab_size). we just swap 1st and 2nd axes\nYoh_swapped = list(Yoh.swapaxes(0,1))","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:25.600591Z","iopub.execute_input":"2021-10-08T15:55:25.600866Z","iopub.status.idle":"2021-10-08T15:55:25.607838Z","shell.execute_reply.started":"2021-10-08T15:55:25.600837Z","shell.execute_reply":"2021-10-08T15:55:25.606850Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"model.fit([Xoh, s0, c0], Yoh_swapped, epochs=300, batch_size=200)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T15:55:29.835967Z","iopub.execute_input":"2021-10-08T15:55:29.836274Z","iopub.status.idle":"2021-10-08T16:11:02.113272Z","shell.execute_reply.started":"2021-10-08T15:55:29.836242Z","shell.execute_reply":"2021-10-08T16:11:02.112580Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating model","metadata":{}},{"cell_type":"code","source":"m = 5000 #testing set size\ntest_dataset, _, _, _ = load_dataset(m)\n_, _, Xoh_test, Yoh_test = preprocess_data(test_dataset, input_vocab, output_vocab, Tx, Ty)\ns0 = np.zeros((m, s_dim))\nc0 = np.zeros((m, s_dim))\nYoh_test = list(Yoh_test.swapaxes(0,1))\npred_test = model.predict([Xoh_test, s0, c0])\nevals = model.evaluate([Xoh_test, s0, c0], Yoh_test)\nprint(\"accuracy: \", np.mean(evals[11:]))","metadata":{"execution":{"iopub.status.busy":"2021-10-08T16:11:02.115072Z","iopub.execute_input":"2021-10-08T16:11:02.115326Z","iopub.status.idle":"2021-10-08T16:11:16.426430Z","shell.execute_reply.started":"2021-10-08T16:11:02.115293Z","shell.execute_reply":"2021-10-08T16:11:16.425722Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"## Formatting custom date","metadata":{}},{"cell_type":"code","source":"s0 = np.zeros((1, s_dim))\nc0 = np.zeros((1, s_dim))\ntest_date = \"2 May 1979\"\ndate = string_to_int(test_date, Tx, input_vocab)\ndate = np.array(date)\ndate = np.expand_dims(date, axis=0)\ndate = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocab)), date)))\npred = model.predict([date, s0, c0])\npred = np.array(pred)\npred = np.argmax(pred, axis = -1)\noutput = [inv_output_vocab[int(i)] for i in pred]\n\nprint(\"output:\", ''.join(output))","metadata":{"execution":{"iopub.status.busy":"2021-10-08T16:11:16.429339Z","iopub.execute_input":"2021-10-08T16:11:16.429546Z","iopub.status.idle":"2021-10-08T16:11:16.496659Z","shell.execute_reply.started":"2021-10-08T16:11:16.429522Z","shell.execute_reply":"2021-10-08T16:11:16.495960Z"},"trusted":true},"execution_count":139,"outputs":[]}]}